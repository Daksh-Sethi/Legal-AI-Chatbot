{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9232263,"sourceType":"datasetVersion","datasetId":5584159},{"sourceId":9232644,"sourceType":"datasetVersion","datasetId":5584411},{"sourceId":9236083,"sourceType":"datasetVersion","datasetId":5586601},{"sourceId":10107179,"sourceType":"datasetVersion","datasetId":6234805},{"sourceId":10178030,"sourceType":"datasetVersion","datasetId":6286676},{"sourceId":212707516,"sourceType":"kernelVersion"}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Initial code","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nimport pandas as pd\nimport numpy as np\nfrom transformers import (\n    BertTokenizer, \n    BertForMaskedLM, \n    BertForQuestionAnswering,\n    Trainer, \n    TrainingArguments,\n    DataCollatorForLanguageModeling\n)\nfrom datasets import Dataset\n\n","metadata":{"_uuid":"1d28f447-2483-4406-82c4-df2aa151a10d","_cell_guid":"089107c4-42df-42ae-9f28-a1eed16536ed","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-12T07:49:45.964809Z","iopub.execute_input":"2024-12-12T07:49:45.965165Z","iopub.status.idle":"2024-12-12T07:50:04.553131Z","shell.execute_reply.started":"2024-12-12T07:49:45.965134Z","shell.execute_reply":"2024-12-12T07:50:04.552411Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_articles(directory_path):\n    \"\"\"\n    Load all text files from a directory\n    \"\"\"\n    articles = []\n    for filename in os.listdir(directory_path):\n        if filename.endswith('.txt'):\n            with open(os.path.join(directory_path, filename), 'r', encoding='utf-8') as file:\n                articles.append(file.read())\n    \n    return articles\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T07:50:04.554612Z","iopub.execute_input":"2024-12-12T07:50:04.555197Z","iopub.status.idle":"2024-12-12T07:50:04.560619Z","shell.execute_reply.started":"2024-12-12T07:50:04.555167Z","shell.execute_reply":"2024-12-12T07:50:04.559596Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def prepare_mlm_dataset(chunks, tokenizer, max_length=512):\n    \"\"\"\n    Prepare dataset for Masked Language Model (MLM) training.\n    \"\"\"\n    encodings = tokenizer(\n        chunks,\n        truncation=True,\n        max_length=max_length,\n        padding=\"max_length\",\n        return_tensors=\"pt\"\n    )\n    \n    # Convert to Dataset\n    dataset = Dataset.from_dict({\n        \"input_ids\": encodings[\"input_ids\"],\n        \"attention_mask\": encodings[\"attention_mask\"],\n        \"labels\": encodings[\"input_ids\"].clone()\n    })\n    return dataset\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T07:50:04.561671Z","iopub.execute_input":"2024-12-12T07:50:04.561895Z","iopub.status.idle":"2024-12-12T07:50:04.579617Z","shell.execute_reply.started":"2024-12-12T07:50:04.561872Z","shell.execute_reply":"2024-12-12T07:50:04.578804Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import BertTokenizer\nfrom datasets import Dataset\nimport pandas as pd\n\ndef prepare_qa_dataset(qa_csv_path, tokenizer, max_length=512):\n    \"\"\"\n    Prepare dataset for Question Answering training.\n    \"\"\"\n    # Load QA data\n    qa_data = pd.read_csv(qa_csv_path)\n\n    # Ensure no empty question or answer\n    qa_data = qa_data.dropna(subset=['Question', 'Answer'])\n    \n    # Tokenize questions and answers\n    encodings = tokenizer(\n        qa_data['Question'].tolist(),  # list of questions\n        qa_data['Answer'].tolist(),  # list of answers\n        truncation=True,\n        max_length=max_length,\n        padding='max_length',\n        return_tensors='pt',  # Returns PyTorch tensors\n        return_token_type_ids=True,\n        return_attention_mask=True\n    )\n\n    # Prepare start and end positions for answers\n    start_positions = []\n    end_positions = []\n\n    for i, (question, answer) in enumerate(zip(qa_data['Question'], qa_data['Answer'])):\n        # Tokenize question and answer together\n        question_answer = tokenizer.encode(question + \" \" + answer, add_special_tokens=True)\n        \n        # Find the position of the answer in the question + answer tokens\n        try:\n            start_idx = question_answer.index(tokenizer.encode(answer, add_special_tokens=False)[0])\n            end_idx = start_idx + len(tokenizer.encode(answer, add_special_tokens=False)) - 1\n        except ValueError:\n            # In case the answer is not found, fallback (e.g., padding or empty tokens)\n            start_idx = 0\n            end_idx = 0\n        \n        start_positions.append(start_idx)\n        end_positions.append(end_idx)\n\n    # Create dataset\n    dataset = Dataset.from_dict({\n        'input_ids': encodings['input_ids'],\n        'attention_mask': encodings['attention_mask'],\n        'start_positions': start_positions,\n        'end_positions': end_positions\n    })\n    \n    return dataset\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T07:50:04.581617Z","iopub.execute_input":"2024-12-12T07:50:04.581907Z","iopub.status.idle":"2024-12-12T07:50:04.590771Z","shell.execute_reply.started":"2024-12-12T07:50:04.581879Z","shell.execute_reply":"2024-12-12T07:50:04.589918Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_mlm(model, train_dataset, tokenizer):\n    \"\"\"\n    Train Masked Language Model\n    \"\"\"\n    # Data collator for MLM\n    data_collator = DataCollatorForLanguageModeling(\n        tokenizer=tokenizer, \n        mlm=True, \n        mlm_probability=0.15\n    )\n    \n    # Training arguments\n    training_args = TrainingArguments(\n        output_dir=\"./mlm_results\",\n        num_train_epochs=10,\n        per_device_train_batch_size=16,\n        save_steps=10_000,\n        save_total_limit=2,\n        logging_dir='./mlm_logs'\n    )\n    \n    # Initialize Trainer\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        data_collator=data_collator\n    )\n    \n    # Train the model\n    trainer.train()\n    \n    return model\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T08:26:41.188411Z","iopub.execute_input":"2024-12-12T08:26:41.189093Z","iopub.status.idle":"2024-12-12T08:26:41.194817Z","shell.execute_reply.started":"2024-12-12T08:26:41.189058Z","shell.execute_reply":"2024-12-12T08:26:41.193831Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def fine_tune_qa(mlm_model, train_dataset):\n    \"\"\"\n    Fine-tune Question Answering model using MLM-trained weights as initialization\n    \"\"\"\n    # Convert MLM model to QA model\n    qa_model = BertForQuestionAnswering.from_pretrained(\n        \"nlpaueb/legal-bert-base-uncased\", \n        state_dict=mlm_model.state_dict()\n    )\n    \n    # Training arguments for QA\n    training_args = TrainingArguments(\n        output_dir=\"./qa_results\",\n        num_train_epochs=10,\n        per_device_train_batch_size=16,\n        save_steps=10_000,\n        save_total_limit=2,\n        learning_rate=2e-5, \n        logging_dir='./qa_logs'\n    )\n    \n    # Initialize Trainer\n    trainer = Trainer(\n        model=qa_model,\n        args=training_args,\n        train_dataset=train_dataset\n    )\n    \n    # Fine-tune the model\n    trainer.train()\n    \n    return qa_model\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T07:51:03.516284Z","iopub.execute_input":"2024-12-12T07:51:03.516631Z","iopub.status.idle":"2024-12-12T07:51:03.52205Z","shell.execute_reply.started":"2024-12-12T07:51:03.516593Z","shell.execute_reply":"2024-12-12T07:51:03.521075Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import BertTokenizerFast","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T07:51:12.02379Z","iopub.execute_input":"2024-12-12T07:51:12.024178Z","iopub.status.idle":"2024-12-12T07:51:12.028688Z","shell.execute_reply.started":"2024-12-12T07:51:12.024145Z","shell.execute_reply":"2024-12-12T07:51:12.027807Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize tokenizer and MLM model\ntokenizer = BertTokenizer.from_pretrained(\"/kaggle/input/output-100mb/trained_tokenizer\")\nmlm_model = BertForMaskedLM.from_pretrained(\"/kaggle/input/output-100mb/trained_mlm_model\")\n\n# Load and prepare articles for MLM\n# articles_dir = '/kaggle/input/legal-data-article/d08'\n\nwith open('/kaggle/input/legal-data-article/d08/output_93.txt', 'r', encoding='utf-8') as file:\n    articles = file.read()\n\nsize=int(len(articles)/5)\narticles=articles[size:3*size]\nchunk_size = 10_000\nchunks = [articles[i:i+chunk_size] for i in range(0, len(articles), chunk_size)]\n\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T08:08:43.495876Z","iopub.execute_input":"2024-12-12T08:08:43.496272Z","iopub.status.idle":"2024-12-12T08:08:56.855725Z","shell.execute_reply.started":"2024-12-12T08:08:43.496238Z","shell.execute_reply":"2024-12-12T08:08:56.854999Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(articles)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T08:09:03.242698Z","iopub.execute_input":"2024-12-12T08:09:03.243002Z","iopub.status.idle":"2024-12-12T08:09:03.249223Z","shell.execute_reply.started":"2024-12-12T08:09:03.242977Z","shell.execute_reply":"2024-12-12T08:09:03.248241Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(chunks)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T08:09:06.506767Z","iopub.execute_input":"2024-12-12T08:09:06.50719Z","iopub.status.idle":"2024-12-12T08:09:06.512855Z","shell.execute_reply.started":"2024-12-12T08:09:06.507155Z","shell.execute_reply":"2024-12-12T08:09:06.51199Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import gc\ndel articles\ngc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T08:09:09.280193Z","iopub.execute_input":"2024-12-12T08:09:09.280507Z","iopub.status.idle":"2024-12-12T08:09:09.718418Z","shell.execute_reply.started":"2024-12-12T08:09:09.280482Z","shell.execute_reply":"2024-12-12T08:09:09.717528Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"    # Prepare MLM dataset\nmlm_dataset = prepare_mlm_dataset(chunks, tokenizer)\ndel chunks\ngc.collect()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T08:25:24.256063Z","iopub.execute_input":"2024-12-12T08:25:24.256401Z","iopub.status.idle":"2024-12-12T08:25:24.26288Z","shell.execute_reply.started":"2024-12-12T08:25:24.256372Z","shell.execute_reply":"2024-12-12T08:25:24.261855Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"    # Train MLM model\ntrained_mlm_model = train_mlm(mlm_model, mlm_dataset, tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T08:26:45.946154Z","iopub.execute_input":"2024-12-12T08:26:45.94646Z","iopub.status.idle":"2024-12-12T12:42:36.923082Z","shell.execute_reply.started":"2024-12-12T08:26:45.946434Z","shell.execute_reply":"2024-12-12T12:42:36.922213Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trained_mlm_model.save_pretrained(\"trained_mlm_model_1\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T12:42:36.924764Z","iopub.execute_input":"2024-12-12T12:42:36.925434Z","iopub.status.idle":"2024-12-12T12:42:37.437811Z","shell.execute_reply.started":"2024-12-12T12:42:36.925393Z","shell.execute_reply":"2024-12-12T12:42:37.43708Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenizer.save_pretrained(\"trained_tokenizer_1\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T12:42:37.438885Z","iopub.execute_input":"2024-12-12T12:42:37.439192Z","iopub.status.idle":"2024-12-12T12:42:37.466263Z","shell.execute_reply.started":"2024-12-12T12:42:37.439163Z","shell.execute_reply":"2024-12-12T12:42:37.465509Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom math import exp\nfrom transformers import DataCollatorForLanguageModeling\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T12:42:37.467772Z","iopub.execute_input":"2024-12-12T12:42:37.467991Z","iopub.status.idle":"2024-12-12T12:42:37.47265Z","shell.execute_reply.started":"2024-12-12T12:42:37.467968Z","shell.execute_reply":"2024-12-12T12:42:37.471666Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_files = [\"/kaggle/input/legal-data-article/d08/output_15.txt\", \"/kaggle/input/legal-data-article/d08/output_25.txt\",\"/kaggle/input/legal-data-article/d08/output_14.txt\"]  # Replace with your test file paths\n\n# Initialize results dictionary\nresults = {\"file\": [], \"mlm_model\": [], \"trained_mlm_model\": []}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T12:42:37.473562Z","iopub.execute_input":"2024-12-12T12:42:37.473805Z","iopub.status.idle":"2024-12-12T12:42:37.482794Z","shell.execute_reply.started":"2024-12-12T12:42:37.473781Z","shell.execute_reply":"2024-12-12T12:42:37.482069Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch\nfrom math import exp\nfrom transformers import DataCollatorForLanguageModeling\n\n\n\n# Ensure that the device is set properly\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Move the models to the selected device (GPU if available, otherwise CPU)\nmlm_model.to(device)\ntrained_mlm_model.to(device)\n\n# You can check if the model is successfully moved to the device like this:\nprint(f\"Model moved to: {device}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T12:42:37.483669Z","iopub.execute_input":"2024-12-12T12:42:37.483891Z","iopub.status.idle":"2024-12-12T12:42:37.496617Z","shell.execute_reply.started":"2024-12-12T12:42:37.483858Z","shell.execute_reply":"2024-12-12T12:42:37.495828Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def evaluate_model_on_txt(model, tokenizer, test_file_path, device):\n    \"\"\"\n    Evaluates a model on a given test file for MLM task and computes perplexity.\n    :param model: MLM model to evaluate\n    :param tokenizer: Tokenizer for tokenizing the input\n    :param test_file_path: Path to the test file (.txt)\n    :param device: Device ('cuda' or 'cpu') where the model should run\n    :return: Perplexity of the model on the test file\n    \"\"\"\n    model.eval()\n    with open(test_file_path, 'r', encoding='utf-8') as file:\n        test_text = file.read()\n\n    # Tokenize the text\n    tokens = tokenizer(test_text, return_tensors=\"pt\", truncation=True, max_length=512, padding=\"max_length\")\n    \n    # Move the tokenized tensors to the specified device (either 'cuda' or 'cpu')\n    input_ids = tokens[\"input_ids\"].to(device)\n    attention_mask = tokens[\"attention_mask\"].to(device)\n\n    # Mask tokens for MLM\n    labels = input_ids.clone()\n    masked_indices = torch.bernoulli(torch.full(labels.shape, 0.15)).bool()  # Mask 15% of tokens\n    labels[~masked_indices] = -100  # Only compute loss on masked tokens\n\n    # Compute loss\n    with torch.no_grad():\n        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs.loss.item()\n\n    # Calculate perplexity\n    perplexity = exp(loss)\n    return perplexity\n\n# Loop through each test file and calculate perplexity for both models\nfor test_file in test_files:\n    print(f\"Evaluating on {test_file}...\")\n    \n    # Pass 'cuda' instead of 'gpu'\n    fine_tuned_perplexity = evaluate_model_on_txt(mlm_model, tokenizer, test_file, \"cuda\")\n    pretrained_perplexity = evaluate_model_on_txt(trained_mlm_model, tokenizer, test_file, \"cuda\")\n    \n    # Append results to the dictionary\n    results[\"file\"].append(test_file)\n    results[\"mlm_model\"].append(fine_tuned_perplexity)\n    results[\"trained_mlm_model\"].append(pretrained_perplexity)\n    \n    # Print the perplexity values for each model\n    print(f\"  mlm_model perplexity: {fine_tuned_perplexity}\")\n    print(f\"  trained_mlm_model perplexity: {pretrained_perplexity}\")\n\n# Convert results to a DataFrame\nresults_df = pd.DataFrame(results)\n\n# Display results\nprint(\"\\nComparison Results:\")\ndisplay(results_df)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T12:42:37.497942Z","iopub.execute_input":"2024-12-12T12:42:37.498588Z","iopub.status.idle":"2024-12-12T12:42:37.88396Z","shell.execute_reply.started":"2024-12-12T12:42:37.498548Z","shell.execute_reply":"2024-12-12T12:42:37.883089Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n    # Prepare QA dataset\n    qa_csv_path = '/kaggle/input/ques-ans2/question_answers2.csv'\n    qa_dataset = prepare_qa_dataset(qa_csv_path, tokenizer)\n    \n    # Fine-tune QA model using MLM model weights\n    trained_qa_model = fine_tune_qa(trained_mlm_model, qa_dataset)\n    \n    # Save models\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T12:42:37.885242Z","iopub.execute_input":"2024-12-12T12:42:37.885901Z","iopub.status.idle":"2024-12-12T14:35:52.556909Z","shell.execute_reply.started":"2024-12-12T12:42:37.88586Z","shell.execute_reply":"2024-12-12T14:35:52.556247Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"    trained_qa_model.save_pretrained(\"trained_qa_model_1\")\n    tokenizer.save_pretrained(\"trained_tokenizer_1\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T14:35:52.557871Z","iopub.execute_input":"2024-12-12T14:35:52.558137Z","iopub.status.idle":"2024-12-12T14:35:53.088495Z","shell.execute_reply.started":"2024-12-12T14:35:52.558111Z","shell.execute_reply":"2024-12-12T14:35:53.087692Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def evaluate_model(question):\n    \"\"\"\n    Predict the answer for a given question without requiring any explicit context.\n    \"\"\"\n    # Use an empty string as the context\n    context = \"\"\n    inputs = tokenizer(question, context, return_tensors=\"pt\", truncation=True, max_length=512)\n    inputs = {key: val.to(\"cuda\") for key, val in inputs.items()}  # Move inputs to GPU\n\n    with torch.no_grad():\n        outputs = trained_qa_model(**inputs)\n\n    # Get start and end logits\n    start_logits = outputs.start_logits\n    end_logits = outputs.end_logits\n\n    # Get predicted start and end indices\n    start_idx = torch.argmax(start_logits, dim=1).item()\n    end_idx = torch.argmax(end_logits, dim=1).item()\n\n    # Decode the predicted answer\n    tokens = inputs[\"input_ids\"][0][start_idx:end_idx + 1]\n    predicted_answer = tokenizer.decode(tokens, skip_special_tokens=True).strip()\n\n    if not predicted_answer:\n        return \"No valid answer found.\"\n    return predicted_answer\n\n# Example usage\nquestion = \"What business laws\"\npredicted_answer = evaluate_model(question)\nprint(\"Predicted Answer:\", predicted_answer)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T14:35:53.092031Z","iopub.execute_input":"2024-12-12T14:35:53.092443Z","iopub.status.idle":"2024-12-12T14:35:53.131274Z","shell.execute_reply.started":"2024-12-12T14:35:53.092416Z","shell.execute_reply":"2024-12-12T14:35:53.130513Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import BertTokenizer, BertForMaskedLM","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T20:41:47.047716Z","iopub.execute_input":"2024-12-17T20:41:47.048480Z","iopub.status.idle":"2024-12-17T20:41:47.060966Z","shell.execute_reply.started":"2024-12-17T20:41:47.048433Z","shell.execute_reply":"2024-12-17T20:41:47.060182Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained(\"/kaggle/input/final-capstone/trained_tokenizer_1\")\nmlm_model = BertForMaskedLM.from_pretrained(\"/kaggle/input/final-capstone/trained_mlm_model_1\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T20:41:47.326704Z","iopub.execute_input":"2024-12-17T20:41:47.327575Z","iopub.status.idle":"2024-12-17T20:41:48.161159Z","shell.execute_reply.started":"2024-12-17T20:41:47.327535Z","shell.execute_reply":"2024-12-17T20:41:48.160126Z"}},"outputs":[{"name":"stderr","text":"BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"from transformers import BertForQuestionAnswering, BertTokenizer\n\n# Load pre-trained BERT model for QA\nmodel = BertForQuestionAnswering.from_pretrained('/kaggle/input/final-capstone/trained_mlm_model_1')\ntokenizer = BertTokenizer.from_pretrained('/kaggle/input/final-capstone/trained_tokenizer_1')\n\ndef generate_answer(model, tokenizer, question, context, max_length=200):\n    # Encode the question and context\n    inputs = tokenizer(\n        question, \n        context, \n        return_tensors=\"pt\", \n        truncation=True, \n        max_length=512\n    )\n    \n    # Get the answer\n    outputs = model(**inputs)\n    start_scores = outputs.start_logits\n    end_scores = outputs.end_logits\n    \n    start_index = torch.argmax(start_scores)\n    end_index = torch.argmax(end_scores)\n    \n    # Extract the answer from the context\n    answer = tokenizer.decode(inputs[\"input_ids\"][0][start_index:end_index+1])\n    return answer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T12:17:55.132440Z","iopub.execute_input":"2024-12-16T12:17:55.133062Z","iopub.status.idle":"2024-12-16T12:17:55.239691Z","shell.execute_reply.started":"2024-12-16T12:17:55.133005Z","shell.execute_reply":"2024-12-16T12:17:55.238730Z"}},"outputs":[{"name":"stderr","text":"Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at /kaggle/input/final-capstone/trained_mlm_model_1 and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"%pip install sentence_transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T12:22:20.246104Z","iopub.execute_input":"2024-12-16T12:22:20.246712Z","iopub.status.idle":"2024-12-16T12:22:29.567368Z","shell.execute_reply.started":"2024-12-16T12:22:20.246678Z","shell.execute_reply":"2024-12-16T12:22:29.566394Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Collecting sentence_transformers\n  Downloading sentence_transformers-3.3.1-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: transformers<5.0.0,>=4.41.0 in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (4.46.3)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (4.66.4)\nRequirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (2.4.0)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (1.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (1.14.1)\nRequirement already satisfied: huggingface-hub>=0.20.0 in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (0.26.2)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (10.3.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2024.6.0)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (6.0.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (3.1.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (1.26.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2024.5.15)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.20.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.4.5)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence_transformers) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence_transformers) (3.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.20.0->sentence_transformers) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2024.6.2)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\nDownloading sentence_transformers-3.3.1-py3-none-any.whl (268 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: sentence_transformers\nSuccessfully installed sentence_transformers-3.3.1\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":53},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\nimport numpy as np\n\ndef find_most_relevant_contexts(df, question, top_k=3):\n    # Load a sentence embedding model\n    model = SentenceTransformer('all-MiniLM-L6-v2')\n    \n    # Embed the question\n    question_embedding = model.encode(question)\n    \n    answer_embeddings = model.encode(df['Answer'].tolist())\n    \n    # Calculate cosine similarities\n    similarities = np.dot(answer_embeddings, question_embedding)\n    \n    top_indices = similarities.argsort()[-top_k:][::-1]\n    \n    return df.iloc[top_indices]['Answer'].tolist()\n\n\ndef prepare_context(df, question):\n    # Find most relevant contexts based on semantic similarity\n    contexts = find_most_relevant_contexts(df, question)\n    return \" \".join(contexts)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T12:22:29.569635Z","iopub.execute_input":"2024-12-16T12:22:29.570058Z","iopub.status.idle":"2024-12-16T12:22:29.643917Z","shell.execute_reply.started":"2024-12-16T12:22:29.569998Z","shell.execute_reply":"2024-12-16T12:22:29.643079Z"}},"outputs":[],"execution_count":54},{"cell_type":"code","source":"def generate_answer(model, tokenizer, question, context, max_length=512):\n    # Encode the question and context\n    inputs = tokenizer(\n        question, \n        context, \n        return_tensors=\"pt\", \n        truncation=True, \n        max_length=max_length\n    )\n    \n    # Get the answer\n    with torch.no_grad():\n        outputs = model(**inputs)\n        start_logits = outputs.start_logits\n        end_logits = outputs.end_logits\n    \n    start_index = torch.argmax(start_logits)\n    end_index = torch.argmax(end_logits)\n    \n    if end_index < start_index:\n        start_index, end_index = end_index, start_index\n    \n    answer_ids = inputs[\"input_ids\"][0][start_index:end_index+1]\n    answer = tokenizer.decode(answer_ids)\n    \n    return answer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T12:22:33.047715Z","iopub.execute_input":"2024-12-16T12:22:33.048433Z","iopub.status.idle":"2024-12-16T12:22:33.054723Z","shell.execute_reply.started":"2024-12-16T12:22:33.048399Z","shell.execute_reply":"2024-12-16T12:22:33.053811Z"}},"outputs":[],"execution_count":55},{"cell_type":"code","source":"# Usage\nquestion = qna_data['Question'][0]\ncontext = prepare_context(qna_data, question)\nanswer = generate_answer(model, tokenizer, question, context)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T12:22:42.514798Z","iopub.execute_input":"2024-12-16T12:22:42.515547Z","iopub.status.idle":"2024-12-16T12:22:51.468799Z","shell.execute_reply.started":"2024-12-16T12:22:42.515513Z","shell.execute_reply":"2024-12-16T12:22:51.468044Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c6d7d8859a5d4b5286789d67f1ea7882"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aedae8c76acb4b088f32da8a860d8cb3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"466d55f915bb4832b3a3039909bc5b7f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"656a14e4359d46589c7684dacba3e607"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a1c63c3c3085449096cda2262f3ac24e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f8214f37f89f4acf921ee24f96b246dc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a07fdb7bd7394b9491331f4b46b6abb7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"00a083a5c4844def8ef739fb00873196"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a09f122c850a42afb3110a257b462373"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"56601246d3014c399845b5470189d25f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f021d4c2ed7479c86e0f8dcf7754bd8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"37af1df16d1845d5b241702c909c1637"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/384 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2c67224a2c7a45b886fc0f27905078a2"}},"metadata":{}}],"execution_count":57},{"cell_type":"code","source":"answer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T12:22:53.523152Z","iopub.execute_input":"2024-12-16T12:22:53.523950Z","iopub.status.idle":"2024-12-16T12:22:53.530373Z","shell.execute_reply.started":"2024-12-16T12:22:53.523917Z","shell.execute_reply":"2024-12-16T12:22:53.529497Z"}},"outputs":[{"execution_count":58,"output_type":"execute_result","data":{"text/plain":"\"leveraging the existing brand's reputation. brand extension is when a\""},"metadata":{}}],"execution_count":58},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Lets try finetuning","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import BertForQuestionAnswering, BertTokenizerFast, AdamW\nfrom sklearn.model_selection import train_test_split\n\nclass QADataset(Dataset):\n    def __init__(self, questions, contexts, answers, tokenizer, max_length=512):\n        self.encodings = tokenizer(\n            questions, \n            contexts, \n            truncation=True, \n            max_length=max_length, \n            padding=True,\n            return_offsets_mapping=True\n        )\n        \n        self.start_positions = []\n        self.end_positions = []\n        \n        for i, (question, context) in enumerate(zip(questions, contexts)):\n            answer = answers[i]\n            \n            # Simple span finding\n            try:\n                answer_start = context.lower().find(answer.lower())\n                if answer_start == -1:\n                    self.start_positions.append(0)\n                    self.end_positions.append(0)\n                    continue\n                \n                answer_end = answer_start + len(answer)\n                \n                # Tokenize context to find token positions\n                context_tokens = tokenizer.encode(context, add_special_tokens=False)\n                answer_tokens = tokenizer.encode(answer, add_special_tokens=False)\n                \n                # Find token positions\n                start_token = len(tokenizer.encode(context[:answer_start], add_special_tokens=False))\n                end_token = start_token + len(answer_tokens) - 1\n                \n                self.start_positions.append(start_token)\n                self.end_positions.append(end_token)\n            \n            except Exception as e:\n                print(f\"Error processing sample: {e}\")\n                self.start_positions.append(0)\n                self.end_positions.append(0)\n    \n    def __len__(self):\n        return len(self.encodings['input_ids'])\n    \n    def __getitem__(self, idx):\n        return {\n            'input_ids': torch.tensor(self.encodings['input_ids'][idx]),\n            'attention_mask': torch.tensor(self.encodings['attention_mask'][idx]),\n            'start_positions': torch.tensor(self.start_positions[idx]),\n            'end_positions': torch.tensor(self.end_positions[idx])\n        }\n\ndef fine_tune_qa_model(\n    df,\n    epochs=3, \n    batch_size=8, \n    learning_rate=5e-5\n):\n    # Prepare model and tokenizer\n    model = BertForQuestionAnswering.from_pretrained('/kaggle/input/final-capstone/trained_mlm_model_1')\n    tokenizer = BertTokenizerFast.from_pretrained('/kaggle/input/final-capstone/trained_tokenizer_1')\n\n    # Split data\n    train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n\n    # Create datasets\n    train_dataset = QADataset(\n        train_df['Question'].tolist(), \n        train_df['Context'].tolist(), \n        train_df['Answer'].tolist(), \n        tokenizer\n    )\n    val_dataset = QADataset(\n        val_df['Question'].tolist(), \n        val_df['Context'].tolist(), \n        val_df['Answer'].tolist(), \n        tokenizer\n    )\n\n    # Create dataloaders\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n\n    # Prepare optimizer\n    optimizer = AdamW(model.parameters(), lr=learning_rate)\n\n    # Move model to device\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model.to(device)\n\n    # Training loop\n    model.train()\n    for epoch in range(epochs):\n        total_loss = 0\n        for batch in train_loader:\n            # Move batch to device\n            batch = {k: v.to(device) for k, v in batch.items()}\n            \n            # Zero grad\n            optimizer.zero_grad()\n            \n            # Forward pass\n            outputs = model(**batch)\n            loss = outputs.loss\n            total_loss += loss.item()\n            \n            # Backward pass\n            loss.backward()\n            optimizer.step()\n        \n        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader)}\")\n    \n    # Save the model\n    model.save_pretrained('./qa_model')\n    tokenizer.save_pretrained('./qa_model')\n    \n    return model, tokenizer\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T12:51:29.313871Z","iopub.execute_input":"2024-12-16T12:51:29.314230Z","iopub.status.idle":"2024-12-16T12:51:29.329117Z","shell.execute_reply.started":"2024-12-16T12:51:29.314198Z","shell.execute_reply":"2024-12-16T12:51:29.327861Z"}},"outputs":[],"execution_count":86},{"cell_type":"code","source":"qna_data = qna_data.head(10)\n# Prepare DataFrame\nqna_data['Context'] = qna_data.apply(\n    lambda row: prepare_context(qna_data, row['Question']), \n    axis=1\n)\n\n# Fine-tune the model\nfinetuned_model, finetuned_tokenizer = fine_tune_qa_model(qna_data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T12:51:29.788078Z","iopub.execute_input":"2024-12-16T12:51:29.788392Z","iopub.status.idle":"2024-12-16T12:51:41.937921Z","shell.execute_reply.started":"2024-12-16T12:51:29.788364Z","shell.execute_reply":"2024-12-16T12:51:41.936979Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4c3c51613a0442786f6a7185c85d001"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b937c3a2cae24e99bc2bb444acf79031"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"035d07f38f6a4cc69b43e4cc7664d88d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"64b0b3562a594f82b5965cf9978b5032"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e4fa4a3a8d944c4bf072cf2c65b785f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"593b0010bfe64ba4b289511b318cdb35"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5053a3f1759641bea289767c13cf61b2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"707d1366f7a040c2b10cc6e39ee53700"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1dcb8986214f43858d725dd2c6bfc28d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"160e62e9bfa945d88ec1c355ca41bcc2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04eb6718f6a843538e62481718ef9323"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e936b02b86b41dab1668d8bda2166b1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c939d372dcbb40d189be8a0209221a9d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"afc6bf10a293490696da8ee92d513eb4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dffa506c6fd7494f89847058ca130f72"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d4e8ebd94e6046a2be01195c60a57dd2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a444388d6e524ddb9cf0154f0d35ec3b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"112fd2bf8e964468865c3abf01aa239d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e3981113868d42048c550a664bb2c9fa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d3be54f9e1f5409380c11d9c67e525e7"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at /kaggle/input/final-capstone/trained_mlm_model_1 and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/3, Loss: 4.769453525543213\nEpoch 2/3, Loss: 4.353943347930908\nEpoch 3/3, Loss: 4.319149017333984\n","output_type":"stream"}],"execution_count":87},{"cell_type":"code","source":"def generate_answer(model, tokenizer, question, context, max_length=512):\n    # Determine the device (CUDA if available, otherwise CPU)\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n    # Move the model to the appropriate device\n    model.to(device)\n    \n    # Encode the question and context\n    inputs = tokenizer(\n        question, \n        context, \n        return_tensors=\"pt\", \n        truncation=True, \n        max_length=max_length\n    ).to(device)  # Move inputs to the same device\n    \n    # Get the answer\n    with torch.no_grad():\n        outputs = model(**inputs)\n        start_logits = outputs.start_logits\n        end_logits = outputs.end_logits\n    \n    # Find the best start and end indices\n    start_index = torch.argmax(start_logits)\n    end_index = torch.argmax(end_logits)\n    \n    # Ensure end_index is after start_index\n    if end_index < start_index:\n        start_index, end_index = end_index, start_index\n    \n    # Extract the answer from the context\n    answer_ids = inputs[\"input_ids\"][0][start_index:end_index+1]\n    answer = tokenizer.decode(answer_ids)\n    \n    return answer\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T12:51:41.939608Z","iopub.execute_input":"2024-12-16T12:51:41.939871Z","iopub.status.idle":"2024-12-16T12:51:41.946853Z","shell.execute_reply.started":"2024-12-16T12:51:41.939846Z","shell.execute_reply":"2024-12-16T12:51:41.945959Z"}},"outputs":[],"execution_count":88},{"cell_type":"code","source":"# Usage\nquestion = qna_data['Question'][0]\ncontext = prepare_context(qna_data, question)\nfinetuned_model.to(device)\nanswer = generate_answer(finetuned_model, finetuned_tokenizer, question, context)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T12:52:42.376973Z","iopub.execute_input":"2024-12-16T12:52:42.377352Z","iopub.status.idle":"2024-12-16T12:52:43.392986Z","shell.execute_reply.started":"2024-12-16T12:52:42.377321Z","shell.execute_reply":"2024-12-16T12:52:43.392351Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e38220bb8f64b4f94d095ccc52441ab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"783b3e9ba9c84931a64dac5b4170eb95"}},"metadata":{}}],"execution_count":92},{"cell_type":"code","source":"answer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T12:52:53.148236Z","iopub.execute_input":"2024-12-16T12:52:53.149094Z","iopub.status.idle":"2024-12-16T12:52:53.154881Z","shell.execute_reply.started":"2024-12-16T12:52:53.149043Z","shell.execute_reply":"2024-12-16T12:52:53.154093Z"}},"outputs":[{"execution_count":94,"output_type":"execute_result","data":{"text/plain":"\"brand extensions. [SEP] brand extensions involve using an established brand to introduce new products or enter new markets, leveraging the existing brand's reputation. 1. establish brand meaning in the minds of the customer2. covert the brand responses to a loyal relationship between the customer and the company3. elicit a customer response brand differentiation is the process of making a brand stand out from its competitors. it's important to attract and retain customers in a crowded marketplace. [SEP]\""},"metadata":{}}],"execution_count":94},{"cell_type":"code","source":"context","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T12:56:47.320474Z","iopub.execute_input":"2024-12-16T12:56:47.320817Z","iopub.status.idle":"2024-12-16T12:56:47.327778Z","shell.execute_reply.started":"2024-12-16T12:56:47.320786Z","shell.execute_reply":"2024-12-16T12:56:47.326827Z"}},"outputs":[{"execution_count":98,"output_type":"execute_result","data":{"text/plain":"\"Brand extensions involve using an established brand to introduce new products or enter new markets, leveraging the existing brand's reputation. 1. Establish brand meaning in the minds of the customer2. Covert the brand responses to a loyal relationship between the customer and the company3. Elicit a customer response Brand differentiation is the process of making a brand stand out from its competitors. It's important to attract and retain customers in a crowded marketplace.\""},"metadata":{}}],"execution_count":98},{"cell_type":"code","source":"print(question, '\\nAnswer'+answer.split('[SEP]')[1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T12:53:12.382395Z","iopub.execute_input":"2024-12-16T12:53:12.382726Z","iopub.status.idle":"2024-12-16T12:53:12.389578Z","shell.execute_reply.started":"2024-12-16T12:53:12.382698Z","shell.execute_reply":"2024-12-16T12:53:12.388677Z"}},"outputs":[{"name":"stdout","text":"Q.1Explain the concept of brand extensions. \nAnswer brand extensions involve using an established brand to introduce new products or enter new markets, leveraging the existing brand's reputation. 1. establish brand meaning in the minds of the customer2. covert the brand responses to a loyal relationship between the customer and the company3. elicit a customer response brand differentiation is the process of making a brand stand out from its competitors. it's important to attract and retain customers in a crowded marketplace. \n","output_type":"stream"}],"execution_count":96},{"cell_type":"markdown","source":"# ****Trying T5 as bert doesn't do text generation****  ","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import T5ForConditionalGeneration, T5Tokenizer, DataCollatorForLanguageModeling, Trainer, TrainingArguments\nfrom datasets import Dataset\n\n# Load pre-trained T5 model and tokenizer\nmodel = T5ForConditionalGeneration.from_pretrained('t5-small')\ntokenizer = T5Tokenizer.from_pretrained('t5-small')\n\n# Read and prepare chunks\nwith open('/kaggle/input/legal-data-article/d08/output_93.txt', 'r', encoding='utf-8') as file:\n    articles = file.read()\n\n# Process the articles\n# size = int(len(articles) / 5)\n# articles = articles[2 * size:]  # Taking a subset of the articles\nchunk_size = 10_000\nchunks = [articles[i:i + chunk_size] for i in range(0, len(articles), chunk_size)]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T22:05:15.013560Z","iopub.execute_input":"2024-12-17T22:05:15.013925Z","iopub.status.idle":"2024-12-17T22:05:53.125004Z","shell.execute_reply.started":"2024-12-17T22:05:15.013890Z","shell.execute_reply":"2024-12-17T22:05:53.123677Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d802356c95449b39efdb2225665ffb7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"970d687891a34185b000e4f1960a80c2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"17959c30fed3488f8a553ad26021fe6e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c65b3ae39af489d825f197c978989a1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3704e3049fd24a9798200244197a1761"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b0cc22157ac54e7fb58dd12961fb183c"}},"metadata":{}},{"name":"stderr","text":"You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# # Prepare MLM dataset\ndef prepare_mlm_dataset(chunks, tokenizer, max_length=512):\n    encodings = tokenizer(\n        chunks,\n        truncation=True,\n        max_length=max_length,\n        padding=\"max_length\",\n        return_tensors=\"pt\"\n    )\n    \n    # Create labels\n    labels = encodings[\"input_ids\"].clone()\n    # Mask some tokens randomly \n    mask_prob = 0.15  \n    mask_indices = torch.rand(labels.shape).lt(mask_prob)\n    labels[mask_indices] = -100\n    \n    dataset = Dataset.from_dict({\n        \"input_ids\": encodings[\"input_ids\"],\n        \"attention_mask\": encodings[\"attention_mask\"],\n        \"labels\": labels\n    })\n    return dataset\n\n\n# Prepare dataset for MLM training\nmlm_dataset = prepare_mlm_dataset(chunks, tokenizer)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T22:05:53.127080Z","iopub.execute_input":"2024-12-17T22:05:53.128009Z","iopub.status.idle":"2024-12-17T22:16:10.593588Z","shell.execute_reply.started":"2024-12-17T22:05:53.127958Z","shell.execute_reply":"2024-12-17T22:16:10.592688Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"\ndel articles\ndel chunks\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T22:16:10.595513Z","iopub.execute_input":"2024-12-17T22:16:10.595865Z","iopub.status.idle":"2024-12-17T22:16:10.873160Z","shell.execute_reply.started":"2024-12-17T22:16:10.595827Z","shell.execute_reply":"2024-12-17T22:16:10.871776Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"\nimport gc\ngc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T22:16:10.876261Z","iopub.execute_input":"2024-12-17T22:16:10.876690Z","iopub.status.idle":"2024-12-17T22:16:11.231852Z","shell.execute_reply.started":"2024-12-17T22:16:10.876644Z","shell.execute_reply":"2024-12-17T22:16:11.230813Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"5"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"\n# Define MLM training function\ndef train_mlm(model, train_dataset, tokenizer):\n    \"\"\"\n    Train Masked Language Model\n    \"\"\"\n    # Training arguments\n    training_args = TrainingArguments(\n        output_dir=\"./t5_mlm_results\",\n        num_train_epochs=10,\n        per_device_train_batch_size=16,\n        logging_steps=5000,\n        save_steps=5000,\n        save_total_limit=2,\n        logging_dir='./t5_mlm_logs',\n        save_strategy=\"steps\",\n        save_only_model=True,\n        \n    )\n    \n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        tokenizer=tokenizer\n    )\n    \n    # Train the model\n    trainer.train()\n    \n    return model\n\n# Train the model on MLM task\ntrained_mlm_model = train_mlm(model, mlm_dataset, tokenizer)\n\n# Save the trained model and tokenizer\ntrained_mlm_model.save_pretrained(\"t5_trained_mlm_model_2\")\ntokenizer.save_pretrained(\"t5_trained_tokenizer_2\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T22:16:11.233065Z","iopub.execute_input":"2024-12-17T22:16:11.233377Z","iopub.status.idle":"2024-12-18T05:33:36.281007Z","shell.execute_reply.started":"2024-12-17T22:16:11.233350Z","shell.execute_reply":"2024-12-18T05:33:36.279895Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_23/1061422713.py:20: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.18.7"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20241217_222332-dqr2s03e</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/thedakshsethi-thapar-university/huggingface/runs/dqr2s03e' target=\"_blank\">./t5_mlm_results</a></strong> to <a href='https://wandb.ai/thedakshsethi-thapar-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/thedakshsethi-thapar-university/huggingface' target=\"_blank\">https://wandb.ai/thedakshsethi-thapar-university/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/thedakshsethi-thapar-university/huggingface/runs/dqr2s03e' target=\"_blank\">https://wandb.ai/thedakshsethi-thapar-university/huggingface/runs/dqr2s03e</a>"},"metadata":{}},{"name":"stderr","text":"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='36250' max='36250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [36250/36250 7:09:59, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>5000</td>\n      <td>0.186300</td>\n    </tr>\n    <tr>\n      <td>10000</td>\n      <td>0.027900</td>\n    </tr>\n    <tr>\n      <td>15000</td>\n      <td>0.016100</td>\n    </tr>\n    <tr>\n      <td>20000</td>\n      <td>0.011700</td>\n    </tr>\n    <tr>\n      <td>25000</td>\n      <td>0.009700</td>\n    </tr>\n    <tr>\n      <td>30000</td>\n      <td>0.008500</td>\n    </tr>\n    <tr>\n      <td>35000</td>\n      <td>0.007800</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"('t5_trained_tokenizer_2/tokenizer_config.json',\n 't5_trained_tokenizer_2/special_tokens_map.json',\n 't5_trained_tokenizer_2/spiece.model',\n 't5_trained_tokenizer_2/added_tokens.json')"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"import torch\n\ntorch.cuda.empty_cache()\ntorch.cuda.reset_peak_memory_stats()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T21:54:40.151424Z","iopub.execute_input":"2024-12-17T21:54:40.151937Z","iopub.status.idle":"2024-12-17T21:54:40.159210Z","shell.execute_reply.started":"2024-12-17T21:54:40.151881Z","shell.execute_reply":"2024-12-17T21:54:40.158097Z"}},"outputs":[],"execution_count":70},{"cell_type":"code","source":"import torch\ntorch.cuda.empty_cache()\ngc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T14:06:16.550165Z","iopub.execute_input":"2024-12-16T14:06:16.550511Z","iopub.status.idle":"2024-12-16T14:06:17.227675Z","shell.execute_reply.started":"2024-12-16T14:06:16.550484Z","shell.execute_reply":"2024-12-16T14:06:17.226534Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"394"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"import torch\nimport gc\n\ndel trained_mlm_model  # Delete the model\ntorch.cuda.empty_cache() \ngc.collect() \n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T14:06:22.170738Z","iopub.execute_input":"2024-12-16T14:06:22.171294Z","iopub.status.idle":"2024-12-16T14:06:22.542250Z","shell.execute_reply.started":"2024-12-16T14:06:22.171261Z","shell.execute_reply":"2024-12-16T14:06:22.541359Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"from transformers import T5ForConditionalGeneration, T5Tokenizer\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T05:54:25.622513Z","iopub.execute_input":"2024-12-18T05:54:25.623358Z","iopub.status.idle":"2024-12-18T05:54:25.628493Z","shell.execute_reply.started":"2024-12-18T05:54:25.623321Z","shell.execute_reply":"2024-12-18T05:54:25.627569Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"import pandas as pd\nqna_data = pd.read_csv('/kaggle/input/ques-ans2/question_answers2.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T05:54:27.189822Z","iopub.execute_input":"2024-12-18T05:54:27.190580Z","iopub.status.idle":"2024-12-18T05:54:27.267751Z","shell.execute_reply.started":"2024-12-18T05:54:27.190542Z","shell.execute_reply":"2024-12-18T05:54:27.266931Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"qna_data['Question'].dtype\nqna_data['Question'] = qna_data['Question'].astype(str)\nqna_data['Answer'] = qna_data['Answer'].astype(str)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T05:54:28.935074Z","iopub.execute_input":"2024-12-18T05:54:28.935883Z","iopub.status.idle":"2024-12-18T05:54:28.944123Z","shell.execute_reply.started":"2024-12-18T05:54:28.935833Z","shell.execute_reply":"2024-12-18T05:54:28.942960Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom transformers import T5ForConditionalGeneration, T5Tokenizer, Trainer, TrainingArguments\nfrom datasets import Dataset\n\n# Tokenizer and model loading\nmodel = T5ForConditionalGeneration.from_pretrained('t5_trained_mlm_model_2')\ntokenizer = T5Tokenizer.from_pretrained('t5_trained_tokenizer_2')\n\ndef tokenize_data(dataset, tokenizer):\n    def tokenize_function(examples):\n        model_inputs = tokenizer(examples['Question'], max_length=512, truncation=True, padding=\"max_length\")\n        with tokenizer.as_target_tokenizer():\n            labels = tokenizer(examples['Answer'], max_length=512, truncation=True, padding=\"max_length\")\n        model_inputs[\"labels\"] = labels[\"input_ids\"]\n        return model_inputs\n\n    return dataset.map(tokenize_function, batched=True)\n\n# Prepare dataset \ntrain_dataset = Dataset.from_pandas(qna_data)\n\n# Tokenize the dataset\ntokenized_train_dataset = tokenize_data(train_dataset, tokenizer)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T05:54:36.299185Z","iopub.execute_input":"2024-12-18T05:54:36.300111Z","iopub.status.idle":"2024-12-18T05:54:43.512141Z","shell.execute_reply.started":"2024-12-18T05:54:36.300074Z","shell.execute_reply":"2024-12-18T05:54:43.511365Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/12263 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f7b3479fc4d4433afc10d4f4bd8774c"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4114: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"\n\n# Define training function\ndef train_t5_model(model, train_dataset, tokenizer):\n    # Training configuration\n    training_args = TrainingArguments(\n        output_dir=\"./t5_qa_model\",\n        num_train_epochs=10,\n        per_device_train_batch_size=16,\n        warmup_steps=100,\n        learning_rate=5e-5,\n        logging_dir='./logs',\n        logging_steps=500,\n        save_steps=1000,\n        save_total_limit=2,\n        save_strategy=\"steps\",\n        save_only_model=True,\n    )\n    \n    # Trainer\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        tokenizer=tokenizer\n    )\n    \n    try:\n        trainer.train()\n    except RuntimeError as e:\n        print(f\"Training error: {e}\")\n        print(\"Trying to reset CUDA memory...\")\n        torch.cuda.empty_cache()\n        raise\n    \n    return model\n\n\n\n# Train the model\ntrained_model = train_t5_model(model, tokenized_train_dataset, tokenizer)\n\n# Save the model and tokenizer\ntrained_model.save_pretrained(\"./t5_qa_finetuned_2\")\ntokenizer.save_pretrained(\"./t5_qa_finetuned_2\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T05:54:43.539722Z","iopub.execute_input":"2024-12-18T05:54:43.539988Z","iopub.status.idle":"2024-12-18T07:26:01.836655Z","shell.execute_reply.started":"2024-12-18T05:54:43.539962Z","shell.execute_reply":"2024-12-18T07:26:01.835820Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_23/943019611.py:19: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='7670' max='7670' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [7670/7670 1:31:16, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>2.081100</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.321400</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.304300</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.298700</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.296600</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.290000</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>0.290500</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>0.284700</td>\n    </tr>\n    <tr>\n      <td>4500</td>\n      <td>0.283600</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>0.282200</td>\n    </tr>\n    <tr>\n      <td>5500</td>\n      <td>0.282300</td>\n    </tr>\n    <tr>\n      <td>6000</td>\n      <td>0.280800</td>\n    </tr>\n    <tr>\n      <td>6500</td>\n      <td>0.276600</td>\n    </tr>\n    <tr>\n      <td>7000</td>\n      <td>0.280400</td>\n    </tr>\n    <tr>\n      <td>7500</td>\n      <td>0.279200</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"('./t5_qa_finetuned_2/tokenizer_config.json',\n './t5_qa_finetuned_2/special_tokens_map.json',\n './t5_qa_finetuned_2/spiece.model',\n './t5_qa_finetuned_2/added_tokens.json')"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"import torch\n\ndef generate_answer(model, tokenizer, input_text, max_length=2048):\n    # Check if GPU is available and set the device\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n    \n    # Tokenize the input text\n    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=max_length, truncation=True)\n    inputs = {key: value.to(device) for key, value in inputs.items()}\n    \n    # Generate the output from the model\n    outputs = model.generate(\n        inputs[\"input_ids\"], \n        max_length=max_length,\n        num_return_sequences=1,\n        no_repeat_ngram_size=2,\n        do_sample=True,\n        top_k=50,\n        top_p=0.95,\n        temperature=0.9\n    )\n    \n    # Decode the generated answer\n    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    return answer\n\n# Example usage:\nquestion = qna_data['Question'][2]\nanswer = generate_answer(trained_model, tokenizer, question, 2048)\nprint('Question: ', question)\nprint('Answer as per data: ', qna_data['Answer'][2])\nprint('Answer by model: ', answer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T07:26:20.464191Z","iopub.execute_input":"2024-12-18T07:26:20.464559Z","iopub.status.idle":"2024-12-18T07:26:21.041384Z","shell.execute_reply.started":"2024-12-18T07:26:20.464527Z","shell.execute_reply":"2024-12-18T07:26:21.040356Z"}},"outputs":[{"name":"stdout","text":"Question:  Q.3How can social media be used in Brand Management?\nAnswer as per data:  Social media platforms can be used to engage with customers, build brand awareness, and gather valuable feedback.\nAnswer by model:  Social media is the most powerful marketing tool to communicate branding and brand behaviors, creating effective campaigns, and optimizing brand exposure.\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"ques='what are negative effect of social media'\nanswer=generate_answer(trained_model, tokenizer, ques, 2048)\nprint(answer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T07:26:32.825749Z","iopub.execute_input":"2024-12-18T07:26:32.826140Z","iopub.status.idle":"2024-12-18T07:26:33.166140Z","shell.execute_reply.started":"2024-12-18T07:26:32.826107Z","shell.execute_reply":"2024-12-18T07:26:33.165305Z"}},"outputs":[{"name":"stdout","text":"Negative effect of social media, are positive effect as social workers are gaining social services and online and offline based marketing.\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"# Trying next word prediciton to get answer","metadata":{}},{"cell_type":"code","source":"model = T5ForConditionalGeneration.from_pretrained('t5_trained_mlm_model_1')\ntokenizer = T5Tokenizer.from_pretrained('t5-small')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T21:23:11.211350Z","iopub.execute_input":"2024-12-17T21:23:11.212177Z","iopub.status.idle":"2024-12-17T21:23:11.757987Z","shell.execute_reply.started":"2024-12-17T21:23:11.212142Z","shell.execute_reply":"2024-12-17T21:23:11.757012Z"}},"outputs":[],"execution_count":51},{"cell_type":"code","source":"import torch\n\n# Ensure the device is consistent\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Move the model to the device\nmodel = model.to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T21:24:34.988125Z","iopub.execute_input":"2024-12-17T21:24:34.988472Z","iopub.status.idle":"2024-12-17T21:24:34.996484Z","shell.execute_reply.started":"2024-12-17T21:24:34.988441Z","shell.execute_reply":"2024-12-17T21:24:34.995752Z"}},"outputs":[],"execution_count":55},{"cell_type":"code","source":"import pandas as pd\nqna_data = pd.read_csv('/kaggle/input/ques-ans2/question_answers2.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T21:24:37.096811Z","iopub.execute_input":"2024-12-17T21:24:37.097204Z","iopub.status.idle":"2024-12-17T21:24:37.161426Z","shell.execute_reply.started":"2024-12-17T21:24:37.097167Z","shell.execute_reply":"2024-12-17T21:24:37.160661Z"}},"outputs":[],"execution_count":56},{"cell_type":"code","source":"qna_data['Question'][0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T21:24:38.858172Z","iopub.execute_input":"2024-12-17T21:24:38.858827Z","iopub.status.idle":"2024-12-17T21:24:38.865489Z","shell.execute_reply.started":"2024-12-17T21:24:38.858790Z","shell.execute_reply":"2024-12-17T21:24:38.864639Z"}},"outputs":[{"execution_count":57,"output_type":"execute_result","data":{"text/plain":"'Q.1Explain the concept of brand extensions.'"},"metadata":{}}],"execution_count":57},{"cell_type":"code","source":"from datasets import Dataset\nqna_data[\"formatted\"] = qna_data.apply(\n    lambda row: f\"<Question> {row['Question']} <Answer> {row['Answer']} <END_ANS>\", axis=1\n)\n\n# Add special tokens\nspecial_tokens = {\"additional_special_tokens\": [\"<Question>\", \"<Answer>\", \"<END_ANS>\"]}\ntokenizer.add_special_tokens(special_tokens)\n# Resize the model's embedding layer \nmodel.resize_token_embeddings(len(tokenizer))\n\n\n# Convert to a Dataset\ndataset = Dataset.from_pandas(qna_data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T21:24:39.700228Z","iopub.execute_input":"2024-12-17T21:24:39.700808Z","iopub.status.idle":"2024-12-17T21:24:39.922713Z","shell.execute_reply.started":"2024-12-17T21:24:39.700770Z","shell.execute_reply":"2024-12-17T21:24:39.921960Z"}},"outputs":[],"execution_count":58},{"cell_type":"code","source":"dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T21:24:40.971403Z","iopub.execute_input":"2024-12-17T21:24:40.972220Z","iopub.status.idle":"2024-12-17T21:24:40.978471Z","shell.execute_reply.started":"2024-12-17T21:24:40.972183Z","shell.execute_reply":"2024-12-17T21:24:40.977636Z"}},"outputs":[{"execution_count":59,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['Question', 'Answer', 'formatted'],\n    num_rows: 12263\n})"},"metadata":{}}],"execution_count":59},{"cell_type":"code","source":"# Tokenize the dataset\ndef tokenize(batch):\n    return tokenizer(batch[\"formatted\"], padding=\"max_length\", truncation=True, max_length=128)\n\ntokenized_dataset = dataset.map(tokenize, batched=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T21:24:42.700115Z","iopub.execute_input":"2024-12-17T21:24:42.700827Z","iopub.status.idle":"2024-12-17T21:24:46.769933Z","shell.execute_reply.started":"2024-12-17T21:24:42.700790Z","shell.execute_reply":"2024-12-17T21:24:46.768931Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/12263 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69d7d92cc7784148a2fbcc15dc612c53"}},"metadata":{}}],"execution_count":60},{"cell_type":"code","source":"import os\n\nos.environ['CUDA_LAUNCH_BLOCKING'] ='1' \nos.environ['TORCH_USE_CUDA_DSA'] = '1'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T21:24:46.801521Z","iopub.execute_input":"2024-12-17T21:24:46.801916Z","iopub.status.idle":"2024-12-17T21:24:46.808271Z","shell.execute_reply.started":"2024-12-17T21:24:46.801872Z","shell.execute_reply":"2024-12-17T21:24:46.807284Z"}},"outputs":[],"execution_count":61},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration, DataCollatorForSeq2Seq, Trainer, TrainingArguments\nfrom datasets import Dataset\n\ndef train_qa(model, train_dataset, tokenizer):\n    # Data collator for sequence-to-sequence tasks\n    data_collator = DataCollatorForSeq2Seq(\n        tokenizer=tokenizer,\n        model=model,\n        padding=True\n    )\n\n    # Modify training arguments for T5 fine-tuning\n    training_args = TrainingArguments(\n        output_dir=\"t5_qa_results\",       # Directory to save checkpoints\n        num_train_epochs=10,              # Number of epochs\n        per_device_train_batch_size=16,   # Batch size\n        learning_rate=5e-5,               # Learning rate\n        weight_decay=0.01,                # Regularization\n        warmup_steps=500,                 # Warm-up steps\n        logging_steps=1000,               # Log progress\n        save_steps=1000,                  # Save model every 1000 steps\n        eval_steps=500,                   # Evaluate every 500 steps\n        save_total_limit=2,               # Keep only the latest 2 checkpoints\n        logging_dir=\"./logs\",             # Logging directory\n        push_to_hub=False\n    )\n\n    # Initialize Trainer for T5\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        data_collator=data_collator,\n        tokenizer=tokenizer\n    )\n\n    # Train the model\n    try:\n        trainer.train()\n    except RuntimeError as e:\n        print(f\"Training error: {e}\")\n        print(\"Trying to reset CUDA memory...\")\n        torch.cuda.empty_cache()\n        raise\n\n    return model\n\n# Load pretrained model and tokenizer\ncheckpoint = \"t5-base\"  # Change to your model checkpoint if needed\ntokenizer = T5Tokenizer.from_pretrained(checkpoint)\nmodel = T5ForConditionalGeneration.from_pretrained(checkpoint)\n\n# Train the model\ntrained_model = train_qa(model, tokenized_dataset, tokenizer)\n\n# Save the model and tokenizer\ntrained_model.save_pretrained(\"./t5_qa_finetuned_gen\")\ntokenizer.save_pretrained(\"./t5_qa_finetuned_gen\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T21:29:45.123051Z","iopub.execute_input":"2024-12-17T21:29:45.123424Z","iopub.status.idle":"2024-12-17T21:29:47.912710Z","shell.execute_reply.started":"2024-12-17T21:29:45.123391Z","shell.execute_reply":"2024-12-17T21:29:47.911384Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_23/564998550.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[64], line 56\u001b[0m\n\u001b[1;32m     53\u001b[0m model \u001b[38;5;241m=\u001b[39m T5ForConditionalGeneration\u001b[38;5;241m.\u001b[39mfrom_pretrained(checkpoint)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m trained_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_qa\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenized_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# Save the model and tokenizer\u001b[39;00m\n\u001b[1;32m     59\u001b[0m trained_model\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./t5_qa_finetuned_gen\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","Cell \u001b[0;32mIn[64], line 41\u001b[0m, in \u001b[0;36mtrain_qa\u001b[0;34m(model, train_dataset, tokenizer)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 41\u001b[0m     \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2123\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2121\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2124\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2128\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2481\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2475\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2476\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2477\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2478\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2479\u001b[0m )\n\u001b[1;32m   2480\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2481\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2483\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2484\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2485\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2486\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2487\u001b[0m ):\n\u001b[1;32m   2488\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2489\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:3579\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3576\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3578\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3579\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3581\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   3582\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3583\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3584\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   3585\u001b[0m ):\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:3633\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3631\u001b[0m         loss_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_items_in_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_items_in_batch\n\u001b[1;32m   3632\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloss_kwargs}\n\u001b[0;32m-> 3633\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3634\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   3635\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   3636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:1891\u001b[0m, in \u001b[0;36mT5ForConditionalGeneration.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1888\u001b[0m         decoder_attention_mask \u001b[38;5;241m=\u001b[39m decoder_attention_mask\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39mfirst_device)\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;66;03m# Decode\u001b[39;00m\n\u001b[0;32m-> 1891\u001b[0m decoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1892\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1893\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1894\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1895\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1896\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1897\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1898\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1899\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1900\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1901\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1902\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1903\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1904\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1905\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1907\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m decoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1909\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:990\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    988\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    989\u001b[0m     err_msg_prefix \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_decoder \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 990\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have to specify either \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merr_msg_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124minput_ids or \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merr_msg_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124minputs_embeds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    992\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_checkpointing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[1;32m    993\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n","\u001b[0;31mValueError\u001b[0m: You have to specify either decoder_input_ids or decoder_inputs_embeds"],"ename":"ValueError","evalue":"You have to specify either decoder_input_ids or decoder_inputs_embeds","output_type":"error"}],"execution_count":64},{"cell_type":"code","source":"def generate_answer(model, tokenizer, question, max_length=200):\n    input_text = f\"<Question> {question} <Answer>\"\n    inputs = tokenizer(input_text, return_tensors=\"pt\", add_special_tokens=True)\n    inputs = {key: value.to(model.device) for key, value in inputs.items()}\n\n    # Improve generation parameters\n    output_ids = model.generate(\n        inputs[\"input_ids\"],\n        max_length=max_length,\n        num_return_sequences=1,\n        do_sample=True,\n        temperature=0.7,\n        top_k=50,\n        top_p=0.95,\n        no_repeat_ngram_size=2,\n        eos_token_id=tokenizer.eos_token_id,\n        pad_token_id=tokenizer.pad_token_id\n    )\n    \n    output_text = tokenizer.decode(output_ids[0], skip_special_tokens=False)\n    # Extract the answer part\n    answer = output_text\n    return answer\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T21:17:08.559783Z","iopub.execute_input":"2024-12-17T21:17:08.560536Z","iopub.status.idle":"2024-12-17T21:17:08.567198Z","shell.execute_reply.started":"2024-12-17T21:17:08.560498Z","shell.execute_reply":"2024-12-17T21:17:08.566373Z"}},"outputs":[],"execution_count":46},{"cell_type":"code","source":"# Example\nquestion = \"Q.1Explain the concept of brand extensions.\"\ngenerated_answer = generate_answer(trained_model, tokenizer, question,200)\nprint(generated_answer)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T21:17:54.945257Z","iopub.execute_input":"2024-12-17T21:17:54.946132Z","iopub.status.idle":"2024-12-17T21:17:57.305112Z","shell.execute_reply.started":"2024-12-17T21:17:54.946096Z","shell.execute_reply":"2024-12-17T21:17:57.303866Z"}},"outputs":[{"name":"stdout","text":"[CLS] <Question> q. 1explain the concept of brand extensions. <Answer> [SEP] q - 1's -'a'or'+ '.'options'mean 2. options used. 3. xting.. 4. 5. or. + / - or 6. = + +. 7 + or + -. ]. [ or ] 7. ( + ) [ + ] [ - ). 6 5 4 6 6 7 ( ) 7 6 9 9 6 8. 9 8 9 7 5 8 8 7 8 5 7 9 5 5 6 4 4 9 3 7 7 1 1 9 4 5 9 1 4 8 4 7 4 3 3 9 2 6 3 5 2 4 2 3 4 1 7 2 8 3 2 9 13 13 14. 13. 14 1313 14 12 13 12. 11 13 11 12 12 11 14 14 11. 12 sales. 17. 18. 19. 16 16. 15. 94 14 15 11 15 16 - 13 23. 88\n","output_type":"stream"}],"execution_count":49},{"cell_type":"code","source":"def generate_answer(model, tokenizer, question, context=\"\", max_length=200):\n    # T5 input formatting\n    input_text = f\"question: {question} context: {context}\"\n    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n    inputs = {key: value.to(model.device) for key, value in inputs.items()}\n\n    # Generation with controlled parameters\n    output_ids = model.generate(\n        inputs[\"input_ids\"],\n        max_length=max_length,\n        num_return_sequences=1,\n        do_sample=False,  # Greedy decoding\n        eos_token_id=tokenizer.eos_token_id,\n        pad_token_id=tokenizer.pad_token_id\n    )\n\n    # Decode and clean up output\n    output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n    return output_text\n\n# Example usage\nquestion = \"Explain the concept of brand extensions.\"\ncontext = \"Brand extensions occur when a company uses an existing brand name to introduce new products or categories. This strategy leverages brand equity to attract customers.\"\ngenerated_answer = generate_answer(model, tokenizer, question, context)\nprint(\"Generated Answer:\", generated_answer)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T21:20:04.279009Z","iopub.execute_input":"2024-12-17T21:20:04.279380Z","iopub.status.idle":"2024-12-17T21:20:06.330488Z","shell.execute_reply.started":"2024-12-17T21:20:04.279351Z","shell.execute_reply":"2024-12-17T21:20:06.329359Z"}},"outputs":[{"name":"stdout","text":"Generated Answer: 1980. [unused8] hazard [unused56] [unused2] [unused8] located [unused44].. [unused6] 422je [unused6] [unused5] [unused41] [unused8] hazard brand [unused5]. strategy leverages brand equity to provisions. [unused6] k [unused17]. [unused4] under 982 leverages brand equity to [unused5]. [unused8]s [unused17] [unused59]. [unused6] 422 [unused56] [unused5]..nono [unused6] [unused52]...no [unused6] 2015 [unused5]..no [unused6] k [unused5] [unused11]...... [unused6] [unused6] [unused52].... assembly [unused6] [unused11] [unused9] [unused9]. sense sense \\ [unused4] [unused20] [unused5].. sense sense [unused9] [unused9] [unused9] [unused4] [unused4]... [unused4]......... $... with... [unused6] preventing. [unused6] $ [unused44] [unused44] he [unused5]. [unused4] brand with.. with : brand brand name [unused6] introduce new [unused15]\n","output_type":"stream"}],"execution_count":50},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}